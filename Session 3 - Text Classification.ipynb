{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [9.0, 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3: Text Classification and Topic Modeling \n",
    "**July 12, 2018**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this session, we'll begin using machine learning techniques to broaden our text analytics. Now that we have a corpus of documents, we can start building topic models to determine if documents are similar to each other and how to categorize our documents broadly. We can also use text classification methods to create tools that allow us to model documents and utterances more precisely. We'll take a look at a specific kind of classification - sentiment analysis and how it can be used for engaging in deeper analytics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to Machine Learning \n",
    "\n",
    "In general, a learning problem considers a set of $n$ instances (or examples) that models are trained on. \n",
    "\n",
    "Instances are represented by a multidimensional entry (aka multivariate data) having several attributes or _features_.\n",
    "\n",
    "The goal of the learning problem is to create a model that predicts a _target_ attribute or feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Feature Space](figures/s3_feature_space.png)\n",
    "\n",
    "The best way to think about machine learning is problems that use mathematical and statistical methods to find patterns in _high dimensional space_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning by Example\n",
    "\n",
    "| Problem Domain                              | Machine Learning Class |\n",
    "|---------------------------------------------|------------------------|\n",
    "| Infer a function from labeled data          | Supervised learning    |\n",
    "| Discover structure of data without feedback | Unsupervised learning  |\n",
    "| Interact with environment towards a goal    | Reinforcement learning |\n",
    "\n",
    "_Given examples (data) extract a meaningful pattern upon which to act._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algorithms by Output \n",
    "\n",
    "| Type of Output                              | Algorithm Category              |\n",
    "|---------------------------------------------|---------------------------------|\n",
    "| **Output is one or more discrete classes**  | **Classification (Supervised**) |\n",
    "| **Output is continuous**                    | **Regression (Supervised)**     |\n",
    "| **Output is membership in a similar group** | **Clustering (Unsupervised)**   |\n",
    "| Output is the distribution of inputs        | Density Estimation              |\n",
    "| Output is simplified from higher dimensions | Dimensionality Reduction        |\n",
    "\n",
    "_Use training data to fit a model which is then used to predict incoming inputs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification \n",
    "\n",
    "![Classification](figures/s3_classification.png)\n",
    "\n",
    "Given labeled input data (with two or more labels), fit a function that can determine for any input, what the label is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression \n",
    "\n",
    "![Regression](figures/s3_regression.png)\n",
    "\n",
    "Given continuous input data fit a function that is able to predict the continuous value of input given other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Clustering \n",
    "\n",
    "![Clustering](figures/s3_clustering.png)\n",
    "\n",
    "Given data, determine a pattern of associated data points or clusters via their similarity or distance from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bag of Words Model \n",
    "\n",
    "In order to do words on machine learning, therefore - we need to represent text numerically somehow.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Bag of Words](figures/s3_bag_of_words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We've already noted that words co-occurring together demonstrate statistical significance that might be related to meaning. The bag-of-words model takes advantage of this to create a model that relies on numeric representations of words that are located close together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectorization \n",
    "\n",
    "Also called feature-extraction; the process of transforming text documents into numeric representations (vectors) that can be used to do machine learning.\n",
    "\n",
    "![Vector Encoding](figures/s3_vector_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The common method of vectorization is to take the _vocabulary_ of the corpus and order them lexicographically (in alphabetical order). To transform a document into a vector, we simply assign a number for each word that represents the word's relationship to the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One Hot Encoding \n",
    "\n",
    "![One Hot Encoding](figures/s3_one_hot_encoding.png)\n",
    "\n",
    "The simplest method is one-hot-encoding where we simply assign a 1 if the word exists in the document, or a 0 otherwise. This is a very common encoding that is generally used in artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequency Encoding \n",
    "\n",
    "![Frequency Encoding](figures/s3_frequency_encoding.png)\n",
    "\n",
    "If we believe that the number of times a word appears in a document matters, we can simply count the number of occurrences and use that number in the word's vector position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF \n",
    "\n",
    "![TF-IDF](figures/s3_tfidf_encoding.png)\n",
    "\n",
    "Term-Frequency, Inverse-Document-Frequency is a measure of a word's relative importance to the document, given its frequency in the entire corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### word2vec \n",
    "\n",
    "![Distributed Representations](figures/s3_distributed_representation.png)\n",
    "\n",
    "The current state of the art is called _word embeddings_. This representation computes a word vector based on word similarity; e.g. \"king\" and \"queen\" will be closer together than \"red\" and \"banana\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling \n",
    "\n",
    "Unsupervised methods of clustering related documents into _topics_ &mdash; e.g. find groups of related documents based on the terms that they contain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Topic Modeling Pipeline](figures/s3_topic_modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification \n",
    "\n",
    "If we have documents labeled with a tag or class, we can train a model of text to identify those categorizations in other texts. E.g. we can detect bullying, positive or negative sentiment, product categories, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Classification Pipeline](figures/s3_classification_pipeline.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
